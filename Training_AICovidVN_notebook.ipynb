{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training_AICovidVN_notebook.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNjAwx90EEBiiRBAOt2sCxW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x74drLHisLMr"},"source":["# 1. Dependencies"]},{"cell_type":"code","metadata":{"id":"iu0vdun_NFyf"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25Wj_KVjPIy6"},"source":["%cd /content/drive/MyDrive/AICovidVN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q_ZEv2jUNGTL"},"source":["!pip install torch torchvision torchaudio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8rG9ymMsWan"},"source":["# 2. Packages"]},{"cell_type":"code","metadata":{"id":"Gmv7jwAasJ-7"},"source":["import torch, torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import torchaudio.transforms as T\n","from torch.utils.data import DataLoader\n","import torch.utils.data.dataset as dataset\n","import pandas as pd\n","import os\n","import torchaudio\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4snIr0z6scew"},"source":["# 3. Dataloader"]},{"cell_type":"code","metadata":{"id":"_RGzZFTqsk0W"},"source":["class AICovidVNDataset(dataset.Dataset):\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.aicovidvn_data = pd.read_csv(csv_file)\n","        self.file_path = self.aicovidvn_data['file_path'].values\n","        self.assessment_result = self.aicovidvn_data['assessment_result'].values\n","        self.root_dir = root_dir\n","\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.aicovidvn_data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        SAMPLE_WAV_PATH = os.path.join(self.root_dir, self.file_path[idx])\n","        waveform, sample_rate = torchaudio.load(SAMPLE_WAV_PATH)\n","        waveform = waveform.to(device)\n","        if self.transform:\n","            waveform = self.transform(waveform)\n","        target = torch.tensor(self.assessment_result[idx], dtype=torch.float32, device=device)\n","        sample = (waveform, target)\n","        return sample"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AE9Fybe2spnh"},"source":["# 4. Training"]},{"cell_type":"markdown","metadata":{"id":"d1Iuj-WMuBR6"},"source":["### 4.1. Applying MFCC transforms to the Data"]},{"cell_type":"code","metadata":{"id":"MoovjHySuAW9"},"source":["mfcc_transform = T.MFCC(\n","    sample_rate=8000,\n","    n_mfcc=256,\n","    melkwargs={\n","        'n_fft': 2048,\n","        'n_mels': 256,\n","        'hop_length': 512,\n","        'mel_scale': 'htk',\n","    }\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjDLNhOuu8DM"},"source":["### 4.2. Load data"]},{"cell_type":"code","metadata":{"id":"pu44UqLPMz4z"},"source":["train_dataset = AICovidVNDataset(csv_file='./Data/aicv115m_public_train/metadata_train_challenge.csv',\n","                                 root_dir='./Data/aicv115m_public_train/train_audio_files_8k',\n","                                 transform=transforms.Compose([\n","                                     mfcc_transform.to(device),\n","                                     transforms.Resize(256).to(device),\n","                                     transforms.CenterCrop(224).to(device)\n","                                 ]))\n","lengths = [int(len(train_dataset) * 0.8), len(train_dataset) - int(len(train_dataset) * 0.8)]\n","train_data, test_data = torch.utils.data.random_split(dataset=train_dataset, lengths=lengths,\n","                                                      generator=torch.Generator().manual_seed(42))\n","\n","\n","\n","batch_size = 64\n","train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n","test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n","\n","train_data_size = len(train_data)\n","test_data_size = len(test_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-NnuYQRvB0h"},"source":["### 4.3. Model"]},{"cell_type":"code","metadata":{"id":"O90IjRmMM9Sg"},"source":["# Load pretrained ResNet50 Model\n","resnet50 = models.resnet50(pretrained=False)\n","resnet50 = resnet50.to(device)\n","resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","# Change the final layer of ResNet50 Model for Transfer Learning\n","fc_inputs = resnet50.fc.in_features\n","\n","resnet50.fc = nn.Sequential(\n","    nn.Linear(fc_inputs, 256),\n","    nn.ReLU(),\n","    nn.Dropout(0.6),\n","    nn.Linear(256, 1),\n","    nn.Sigmoid()\n",")\n","\n","# Convert model to be used on GPU\n","resnet50 = resnet50.to(device)\n","\n","# Define Optimizer and Loss Function\n","loss_func = nn.BCELoss()\n","num_epochs = 500\n","optimizer = optim.Adam(resnet50.parameters(), lr=0.01)\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)\n","trained_model, history, best_epoch = train_and_validate(resnet50, loss_func, optimizer, scheduler, num_epochs)\n","torch.save(history, 'history.pt')"],"execution_count":null,"outputs":[]}]}