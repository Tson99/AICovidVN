{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Training_AICovidVN_notebook.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO+qhtUaSQC1m7JyUlBkVjV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x74drLHisLMr"},"source":["# 1. Dependencies"]},{"cell_type":"code","metadata":{"id":"iu0vdun_NFyf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624889094858,"user_tz":-420,"elapsed":16051,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}},"outputId":"def03671-7a6b-484b-cc61-879ae6889cda"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"25Wj_KVjPIy6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624889094860,"user_tz":-420,"elapsed":18,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}},"outputId":"05cd7588-487a-4d7a-f4c4-9e60d65477c6"},"source":["%cd /content/drive/MyDrive/AICovidVN"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/AICovidVN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q_ZEv2jUNGTL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624889099347,"user_tz":-420,"elapsed":4499,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}},"outputId":"1abf6782-682e-4281-c3ef-e31c66dd687c"},"source":["!pip install torch torchvision torchaudio"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.10.0+cu102)\n","Collecting torchaudio\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/20/eab40caad8f4b97f5e91a5de8ba5ec29115e08fa4c9a808725490b7b4844/torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n","\u001b[K     |████████████████████████████████| 1.9MB 4.0MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n","Installing collected packages: torchaudio\n","Successfully installed torchaudio-0.9.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Z8rG9ymMsWan"},"source":["# 2. Packages"]},{"cell_type":"code","metadata":{"id":"Gmv7jwAasJ-7","executionInfo":{"status":"ok","timestamp":1624889104022,"user_tz":-420,"elapsed":4681,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}}},"source":["import torch, torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import torchaudio.transforms as T\n","from torch.utils.data import DataLoader\n","import torch.utils.data.dataset as dataset\n","import pandas as pd\n","import os\n","import torchaudio\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4snIr0z6scew"},"source":["# 3. Dataloader"]},{"cell_type":"code","metadata":{"id":"_RGzZFTqsk0W","executionInfo":{"status":"ok","timestamp":1624889104031,"user_tz":-420,"elapsed":16,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}}},"source":["class AICovidVNDataset(dataset.Dataset):\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.aicovidvn_data = pd.read_csv(csv_file)\n","        self.file_path = self.aicovidvn_data['file_path'].values\n","        self.assessment_result = self.aicovidvn_data['assessment_result'].values\n","        self.root_dir = root_dir\n","\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.aicovidvn_data)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        SAMPLE_WAV_PATH = os.path.join(self.root_dir, self.file_path[idx])\n","        waveform, sample_rate = torchaudio.load(SAMPLE_WAV_PATH)\n","        waveform = waveform.to(device)\n","        if self.transform:\n","            waveform = self.transform(waveform)\n","        target = torch.tensor(self.assessment_result[idx], dtype=torch.float32, device=device)\n","        sample = (waveform, target)\n","        return sample"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AE9Fybe2spnh"},"source":["# 4. Training"]},{"cell_type":"markdown","metadata":{"id":"d1Iuj-WMuBR6"},"source":["### 4.1. Applying MFCC transforms to the Data"]},{"cell_type":"code","metadata":{"id":"MoovjHySuAW9","executionInfo":{"status":"ok","timestamp":1624889104033,"user_tz":-420,"elapsed":15,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}}},"source":["mfcc_transform = T.MFCC(\n","    sample_rate=8000,\n","    n_mfcc=256,\n","    melkwargs={\n","        'n_fft': 2048,\n","        'n_mels': 256,\n","        'hop_length': 512,\n","        'mel_scale': 'htk',\n","    }\n",")"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjDLNhOuu8DM"},"source":["### 4.2. Load data"]},{"cell_type":"code","metadata":{"id":"pu44UqLPMz4z","executionInfo":{"status":"ok","timestamp":1624889106078,"user_tz":-420,"elapsed":2059,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}}},"source":["train_dataset = AICovidVNDataset(csv_file='./Data/aicv115m_public_train/metadata_train_challenge.csv',\n","                                 root_dir='./Data/aicv115m_public_train/train_audio_files_8k',\n","                                 transform=transforms.Compose([\n","                                     mfcc_transform.to(device),\n","                                     transforms.Resize(256).to(device),\n","                                     transforms.CenterCrop(224).to(device)\n","                                 ]))\n","lengths = [int(len(train_dataset) * 0.8), len(train_dataset) - int(len(train_dataset) * 0.8)]\n","train_data, test_data = torch.utils.data.random_split(dataset=train_dataset, lengths=lengths,\n","                                                      generator=torch.Generator().manual_seed(42))\n","\n","\n","\n","batch_size = 64\n","train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n","test_data_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n","\n","train_data_size = len(train_data)\n","test_data_size = len(test_data)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-NnuYQRvB0h"},"source":["### 4.3. Model"]},{"cell_type":"code","metadata":{"id":"H6Lc1DHY7j8o","executionInfo":{"status":"ok","timestamp":1624889106079,"user_tz":-420,"elapsed":7,"user":{"displayName":"Sơn Đỗ Tuấn","photoUrl":"","userId":"06382804501314679295"}}},"source":["def train_and_validate(model, loss_criterion, optimizer, scheduler, epochs=25):\n","    '''\n","    Function to train and validate\n","    Parameters\n","        :param model: Model to train and validate\n","        :param loss_criterion: Loss Criterion to minimize\n","        :param optimizer: Optimizer for computing gradients\n","        :param epochs: Number of epochs (default=25)\n","\n","    Returns\n","        model: Trained Model with best validation accuracy\n","        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n","    '''\n","\n","    start = time.time()\n","    history = []\n","    best_loss = 100000.0\n","    best_epoch = None\n","\n","    for epoch in range(epochs):\n","        epoch_start = time.time()\n","        print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n","\n","        # Set to training mode\n","        model.train()\n","\n","        # Loss and Accuracy within the epoch\n","        train_loss = 0.0\n","        train_acc = 0.0\n","\n","        valid_loss = 0.0\n","        valid_acc = 0.0\n","\n","        for i, (inputs, labels) in enumerate(train_data_loader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # Clean existing gradients\n","            optimizer.zero_grad()\n","\n","            # Forward pass - compute outputs on input data using the model\n","            outputs = model(inputs)\n","            outputs = torch.squeeze(outputs, dim=1)\n","            sigmoid = nn.Sigmoid()\n","            outputs = sigmoid(outputs)\n","\n","            # Compute losss\n","            loss = loss_criterion(outputs, labels)\n","\n","            # Backpropagate the gradients\n","            loss.backward()\n","\n","            # Update the parameters\n","            optimizer.step()\n","            scheduler.step()\n","\n","            # Compute the total loss for the batch and add it to train_loss\n","            train_loss += loss.item() * inputs.size(0)\n","\n","            # Compute the accuracy\n","            predictions = outputs >= 0.5\n","            # ret, predictions = torch.max(outputs.data, 1)\n","            correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","            # Convert correct_counts to float and then compute the mean\n","\n","            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","            # Compute total accuracy in the whole batch and add to train_acc\n","            train_acc += acc.item() * inputs.size(0)\n","\n","            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n","\n","        # Validation - No gradient tracking needed\n","        with torch.no_grad():\n","\n","            # Set to evaluation mode\n","            model.eval()\n","\n","            # Validation loop\n","            for j, (inputs, labels) in enumerate(test_data_loader):\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # Forward pass - compute outputs on input data using the model\n","                outputs = model(inputs)\n","\n","                # Compute loss\n","                loss = loss_criterion(outputs, labels)\n","\n","                # Compute the total loss for the batch and add it to valid_loss\n","                valid_loss += loss.item() * inputs.size(0)\n","\n","                # Calculate validation accuracy\n","                # ret, predictions = torch.max(outputs.data, 1)\n","                predictions = outputs >= 0.5\n","                correct_counts = predictions.eq(labels.data.view_as(predictions))\n","\n","                # Convert correct_counts to float and then compute the mean\n","                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n","\n","                # Compute total accuracy in the whole batch and add to valid_acc\n","                valid_acc += acc.item() * inputs.size(0)\n","                # if not j % 100:\n","                print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j,\n","                                                                                                           loss.item(),\n","                                                                                                           acc.item()))\n","        if valid_loss < best_loss:\n","            best_loss = valid_loss\n","            best_epoch = epoch\n","\n","        # Find average training loss and training accuracy\n","        avg_train_loss = train_loss / train_data_size\n","        avg_train_acc = train_acc / train_data_size\n","\n","        # Find average training loss and training accuracy\n","        avg_valid_loss = valid_loss / test_data_size\n","        avg_valid_acc = valid_acc / test_data_size\n","\n","        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n","\n","        epoch_end = time.time()\n","\n","        print(\n","            \"Epoch : {:03d}, Training: Loss - {:.4f}, Accuracy - {:.4f}%, \\n\\t\\tValidation : Loss - {:.4f}, Accuracy - {:.4f}%, Time: {:.4f}s\".format(\n","                epoch + 1, avg_train_loss, avg_train_acc * 100, avg_valid_loss, avg_valid_acc * 100,\n","                epoch_end - epoch_start))\n","\n","        # Save if the model has best accuracy till now\n","        if (epoch + 1) % 50:\n","            torch.save(model, 'Models/model_' + str(epoch) + '.pt')\n","    return model, history, best_epoch"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QXZ53KKy8c8c"},"source":["### 4.4. Training"]},{"cell_type":"code","metadata":{"id":"O90IjRmMM9Sg"},"source":["# Load pretrained ResNet50 Model\n","resnet50 = models.resnet50(pretrained=False)\n","resnet50 = resnet50.to(device)\n","resnet50.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","# Change the final layer of ResNet50 Model for Transfer Learning\n","fc_inputs = resnet50.fc.in_features\n","\n","resnet50.fc = nn.Sequential(\n","    nn.Linear(fc_inputs, 256),\n","    nn.ReLU(),\n","    nn.Dropout(0.6),\n","    nn.Linear(256, 1),\n","    nn.Sigmoid()\n",")\n","\n","# Convert model to be used on GPU\n","resnet50 = resnet50.to(device)\n","\n","# Define Optimizer and Loss Function\n","loss_func = nn.BCELoss()\n","num_epochs = 500\n","optimizer = optim.Adam(resnet50.parameters(), lr=0.01)\n","scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)\n","trained_model, history, best_epoch = train_and_validate(resnet50, loss_func, optimizer, scheduler, num_epochs)\n","torch.save(history, 'history.pt')"],"execution_count":null,"outputs":[]}]}